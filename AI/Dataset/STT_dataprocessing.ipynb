{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b93933-a38b-46e7-8d70-2fb9099989f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T13:09:16.218864Z",
     "iopub.status.busy": "2025-03-27T13:09:16.218576Z",
     "iopub.status.idle": "2025-03-27T13:09:16.570371Z",
     "shell.execute_reply": "2025-03-27T13:09:16.569622Z",
     "shell.execute_reply.started": "2025-03-27T13:09:16.218837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 27 22:09:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   58C    P0            126W /  230W |   12208MiB /  24564MiB |     74%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04850ec2-4356-4bf7-ae2b-aa965c13ef24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T12:33:07.218476Z",
     "iopub.status.busy": "2025-03-17T12:33:07.218150Z",
     "iopub.status.idle": "2025-03-17T12:33:07.227655Z",
     "shell.execute_reply": "2025-03-17T12:33:07.226874Z",
     "shell.execute_reply.started": "2025-03-17T12:33:07.218448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78755aec-c8d0-4c24-a33b-4120bfec9726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T12:52:45.256601Z",
     "iopub.status.busy": "2025-03-17T12:52:45.256130Z",
     "iopub.status.idle": "2025-03-17T12:52:56.436465Z",
     "shell.execute_reply": "2025-03-17T12:52:56.435639Z",
     "shell.execute_reply.started": "2025-03-17T12:52:45.256551Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://repo.ai.gato/registry/repository/pypi-proxy/simple\n",
      "Collecting transformers==4.40.0\n",
      "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/transformers/4.40.0/transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "Requirement already satisfied: accelerate in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: gradio in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (4.26.0)\n",
      "Requirement already satisfied: filelock in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers==4.40.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n",
      "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/tokenizers/0.19.1/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers==4.40.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers==4.40.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gradio) (5.5.0)\n",
      "Requirement already satisfied: fastapi in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.115.10)\n",
      "Requirement already satisfied: ffmpy in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==0.15.1 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/markupsafe/2.1.5/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gradio) (3.10.1)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio)\n",
      "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/pillow/10.4.0/pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (2.10.6)\n",
      "Requirement already satisfied: pydub in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.0.18)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.9.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/tomlkit/0.12.0/tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gradio-client==0.15.1->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gradio-client==0.15.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (1.30.0)\n",
      "Requirement already satisfied: anyio in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.8.0)\n",
      "Requirement already satisfied: certifi in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (13.9.4)\n",
      "\u001b[33mWARNING: typer 0.15.2 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from fastapi->gradio) (0.41.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from requests->transformers==4.40.0) (1.26.20)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (2.19.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio) (0.1.2)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to MarkupSafe in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.40.0 accelerate gradio --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46877bed-9a63-4f31-87e9-52724d2bd6c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:28.776221Z",
     "iopub.status.busy": "2025-03-23T07:16:28.775798Z",
     "iopub.status.idle": "2025-03-23T07:49:36.339706Z",
     "shell.execute_reply": "2025-03-23T07:49:35.585027Z",
     "shell.execute_reply.started": "2025-03-23T07:16:28.776187Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdff29f24dec43018b5d4b53ba0451dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)i/whisper-small/resolve/main/config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3445e189781e4033a36ba65d84bcd973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)per-small/resolve/main/model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de26cefa3564857a3ea5d72aeabd28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)mall/resolve/main/generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054b520d54e448c5b192939d0edf92bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ll/resolve/main/preprocessor_config.json:   0%|          | 0.00/185k [00:07<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7771ea2f6b7049ce941e42b51bfe8b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)small/resolve/main/tokenizer_config.json:   0%|          | 0.00/283k [00:08<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc8088f8b51454e9ca4b0cbcba657c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ai/whisper-small/resolve/main/vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53849cb2904c49b597e07949164b1f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)hisper-small/resolve/main/tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fe051b770047eb9dd7a0faf636f8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ai/whisper-small/resolve/main/merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e529f6d90c4e5eba0ac4b1df8c84f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)isper-small/resolve/main/normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baea0e580d02452682ca75ffab3c066c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)per-small/resolve/main/added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46b2174a41c480f80f6b8b4f84b9f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)all/resolve/main/special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which requires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 음성 파일 → 텍스트 변환\u001b[39;00m\n\u001b[1;32m     23\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyeongsangdo1.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m stt_result \u001b[38;5;241m=\u001b[39m \u001b[43mstt_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m user_input_text \u001b[38;5;241m=\u001b[39m stt_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisper 변환 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input_text)\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:283\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1360\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1274\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1275\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:521\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    519\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 521\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:597\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# 3. Make sure generation config is correctly set\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# Make sure the generation config is correctly set depending on whether timestamps are to be returned or not\u001b[39;00m\n\u001b[1;32m    591\u001b[0m return_dict_in_generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_return_outputs(\n\u001b[1;32m    592\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m    593\u001b[0m     return_token_timestamps\u001b[38;5;241m=\u001b[39mreturn_token_timestamps,\n\u001b[1;32m    594\u001b[0m     logprob_threshold\u001b[38;5;241m=\u001b[39mlogprob_threshold,\n\u001b[1;32m    595\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    596\u001b[0m )\n\u001b[0;32m--> 597\u001b[0m timestamp_begin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_return_timestamps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_timestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_language_and_task(\n\u001b[1;32m    601\u001b[0m     language\u001b[38;5;241m=\u001b[39mlanguage, task\u001b[38;5;241m=\u001b[39mtask, is_multilingual\u001b[38;5;241m=\u001b[39mis_multilingual, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config\n\u001b[1;32m    602\u001b[0m )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_num_frames(\n\u001b[1;32m    604\u001b[0m     return_token_timestamps\u001b[38;5;241m=\u001b[39mreturn_token_timestamps, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    605\u001b[0m )\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:1307\u001b[0m, in \u001b[0;36mWhisperGenerationMixin._set_return_timestamps\u001b[0;34m(self, return_timestamps, is_shortform, generation_config)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_shortform:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 1307\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1310\u001b[0m         )\n\u001b[1;32m   1312\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting `return_timestamps=True` for long-form generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1313\u001b[0m     return_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: You have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which requires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "### 1. STT 모델 (Whisper) 로드 및 실행\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Whisper 모델 로드\n",
    "stt_model_id = \"openai/whisper-small\"\n",
    "stt_model = AutoModelForSpeechSeq2Seq.from_pretrained(stt_model_id).to(device)\n",
    "stt_processor = AutoProcessor.from_pretrained(stt_model_id)\n",
    "\n",
    "# ASR 파이프라인 설정\n",
    "stt_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=stt_model,\n",
    "    tokenizer=stt_processor.tokenizer,\n",
    "    feature_extractor=stt_processor.feature_extractor,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 음성 파일 → 텍스트 변환\n",
    "audio_file = \"gyeongsangdo1.wav\"\n",
    "stt_result = stt_pipe(audio_file)\n",
    "user_input_text = stt_result[\"text\"]\n",
    "\n",
    "print(\"Whisper 변환 결과:\", user_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243afa04-e809-483c-bfc5-d4bc06a06ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T13:09:47.769759Z",
     "iopub.status.busy": "2025-03-27T13:09:47.769455Z",
     "iopub.status.idle": "2025-03-27T13:10:01.073407Z",
     "shell.execute_reply": "2025-03-27T13:10:01.072676Z",
     "shell.execute_reply.started": "2025-03-27T13:09:47.769735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://repo.ai.gato/registry/repository/pypi-proxy/simple\n",
      "Requirement already satisfied: pydub in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (0.25.1)\n",
      "Reading package lists... Done\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub\n",
    "!apt-get update && apt-get install -y ffmpeg  # 리눅스일 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a71b17-4211-4b0a-bebf-0ac66836123e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T13:11:31.951614Z",
     "iopub.status.busy": "2025-03-27T13:11:31.951307Z",
     "iopub.status.idle": "2025-03-27T13:11:32.008680Z",
     "shell.execute_reply": "2025-03-27T13:11:32.007854Z",
     "shell.execute_reply.started": "2025-03-27T13:11:31.951589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# 파일 경로\n",
    "wav1_path = \"Gyeongsangdo/wavs/say_set1_collectorgs212_speakergs2388_42_0_29_5.wav\"\n",
    "wav2_path = \"Gyeongsangdo/wavs/say_set1_collectorgs212_speakergs2388_42_0_29_6.wav\"\n",
    "\n",
    "# 오디오 로딩\n",
    "audio1 = AudioSegment.from_wav(wav1_path)\n",
    "audio2 = AudioSegment.from_wav(wav2_path)\n",
    "\n",
    "# 이어 붙이기\n",
    "combined = audio1 + audio2\n",
    "\n",
    "# 저장\n",
    "combined.export(\"Speaker.wav\", format=\"wav\")\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23048429-18df-4886-8c73-98cbac680f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T16:58:06.481503Z",
     "iopub.status.busy": "2025-03-29T16:58:06.481227Z",
     "iopub.status.idle": "2025-03-29T16:58:08.696149Z",
     "shell.execute_reply": "2025-03-29T16:58:08.695521Z",
     "shell.execute_reply.started": "2025-03-29T16:58:06.481480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ metadata.txt 확장자 제거 완료\n"
     ]
    }
   ],
   "source": [
    "# 확장자 제거 자동화 (백업 먼저 해놓기!)\n",
    "with open(\"/home/20203068/Gyeongsangdo/metadata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "fixed_lines = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(\"|\")\n",
    "    if parts[0].endswith(\".wav\"):\n",
    "        parts[0] = parts[0].replace(\".wav\", \"\")\n",
    "    fixed_lines.append(\"|\".join(parts) + \"\\n\")\n",
    "\n",
    "with open(\"/home/20203068/Gyeongsangdo/metadata.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(fixed_lines)\n",
    "\n",
    "print(\"✅ metadata.txt 확장자 제거 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e80063-dee3-4120-ae37-09f2fe2bfac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.6.0 (py3.10)",
   "language": "python",
   "name": "pytorch-2.6.0-cuda12.4-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
